{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we design and implement a real-time payment data pipeline for Darooghe, a payment service provider facilitating transactions through an online gateway, POS devices, mobile applications, and NFC (contactless) payments. The primary objectives are to ingest transaction events at scale, validate and process them in real time, detect and flag fraudulent activity, compute commission metrics, and store both raw and aggregated data for historical analysis and visualization. This architecture ensures high-throughput, low-latency handling of payment events, enabling Darooghe to monitor transaction flows and business insights continuously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Kafka\n",
    "Apache Kafka is an open-source distributed event streaming platform used here as the ingestion layer. It provides a durable, fault-tolerant backbone for capturing high volumes of transaction events from a synthetic transaction generator. Transactions are published to Kafka topics (darooghe.transactions), where they can be consumed by downstream systems. Kafka’s scalability and partitioning model ensure that our pipeline can handle spikes in transaction rates with minimal latency\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"additional_files\\image1.png\" alt=\"Roulette Wheel\" style=\"width: 50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark\n",
    "Apache Spark is a unified analytics engine for large-scale data processing. In this project, we employ both Spark Streaming for real-time (micro-batch) processing of incoming Kafka events and PySpark for batch analytics on stored data. Spark Streaming reads from Kafka, performs windowed aggregations, applies fraud-detection rules, and calculates commission metrics on the fly. PySpark batch jobs analyze historical transaction data to generate reports on commission efficiency, temporal patterns, and customer segmentation\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"additional_files\\Picture5-2.png\" alt=\"Roulette Wheel\" style=\"width: 50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB\n",
    "MongoDB is a NoSQL, document-oriented database chosen for its flexible schema and horizontal scalability. We use MongoDB to store validated transactions and aggregated insights. A partitioning strategy (e.g., by date or merchant) is employed to optimize query performance and data retention policies. Aggregated collections (daily, weekly, monthly summaries) support efficient historical analysis and dashboard visualizations without the overhead of re-processing raw event streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import logging\n",
    "from confluent_kafka import Consumer, Producer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as _sum, avg, count, col, udf, stddev, hour, date_format, when, to_date\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize the local Kafka-based ingestion pipeline and monitoring UI, follow these steps:\n",
    "\n",
    "1. **Start ZooKeeper**  \n",
    "\n",
    "   Navigate to your Kafka installation directory and run:  \n",
    "\n",
    "   ```bash\n",
    "   bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "   ```\n",
    "\n",
    "2. **Start Kafka Broker**  \n",
    "\n",
    "   In a new terminal (still inside the Kafka directory), launch the broker:  \n",
    "   \n",
    "   ```bash\n",
    "   bin/kafka-server-start.sh config/server.properties\n",
    "   ```\n",
    "\n",
    "3. **Launch Kafdrop Monitoring UI**  \n",
    "\n",
    "   Open a separate terminal, navigate to your Kafdrop installation folder, and execute:  \n",
    "   \n",
    "   ```bash\n",
    "   java -jar kafdrop-4.1.0.jar --kafka.brokerConnect=localhost:9092\n",
    "   ```\n",
    "\n",
    "4. **Access the UI**  \n",
    "\n",
    "   Open your browser and go to  \n",
    "\n",
    "   ```\n",
    "   http://localhost:9000\n",
    "   ```  \n",
    "   Here you can explore all Kafka topics, view partitions, offsets, and inspect individual messages.\n",
    "\n",
    "5. **Run the Transaction Generator**  \n",
    "\n",
    "   Finally, execute your Python producer script to start generating synthetic transactions: \n",
    "\n",
    "   ```bash\n",
    "   python darooghe_pulse.py\n",
    "   \n",
    "   ```  \n",
    "   As messages are produced to the `darooghe.transactions` topic, you’ll see them appear in real time within the Kafdrop UI—complete with headers, payloads, and partition metadata.\n",
    "\n",
    "   <div style=\"text-align: center;\">\n",
    "      <img src=\"additional_files\\Screenshot from 2025-04-26 14-11-41.png\" alt=\"Roulette Wheel\" style=\"width: 70%;\">\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Consumer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_transaction(event):\n",
    "    errors = []\n",
    "    try:\n",
    "        total_expected = event['amount'] + event['vat_amount'] + event['commission_amount']\n",
    "        if event['total_amount'] != total_expected:\n",
    "            errors.append('ERR_AMOUNT')\n",
    "    except:\n",
    "        errors.append('ERR_AMOUNT')\n",
    "\n",
    "    try:\n",
    "        now = datetime.datetime.utcnow()\n",
    "        ts = datetime.datetime.fromisoformat(event['timestamp'].replace(\"Z\", \"\"))\n",
    "        day_per_sec = 86400\n",
    "        if ts > now or (now - ts).total_seconds() > day_per_sec:\n",
    "            errors.append('ERR_TIME')\n",
    "    except:\n",
    "        errors.append('ERR_TIME')\n",
    "\n",
    "    try:\n",
    "        if event['payment_method'] == 'mobile':\n",
    "            os = event.get('device_info', {}).get('os', None)\n",
    "            if os not in ['Android', 'iOS']:\n",
    "                errors.append('ERR_DEVICE')\n",
    "    except:\n",
    "        errors.append('ERR_DEVICE')\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "consumer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'darooghe-consumer',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "}\n",
    "consumer = Consumer(consumer_conf)\n",
    "consumer.subscribe(['darooghe.transactions'])\n",
    "\n",
    "producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            logging.error(f\"Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            event = json.loads(msg.value().decode('utf-8'))\n",
    "            # raw_event = json.loads(msg.value().decode('utf-8'))\n",
    "            # event = parse_transaction(raw_event)\n",
    "            transaction_id = event.get('transaction_id', 'unknown')\n",
    "\n",
    "            errors = validate_transaction(event)\n",
    "\n",
    "            if errors:\n",
    "                error_msg = {\n",
    "                    'transaction_id': transaction_id,\n",
    "                    'errors': errors,\n",
    "                    'original_data': event\n",
    "                }\n",
    "                producer.produce(\n",
    "                    'darooghe.error_logs',\n",
    "                    key=transaction_id,\n",
    "                    value=json.dumps(error_msg, default=str)\n",
    "                )\n",
    "                logging.warning(f\"Invalid transaction: {transaction_id}, errors: {errors}\")\n",
    "            else:\n",
    "                logging.info(f\"Valid transaction: {transaction_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Parse error: {e}\")\n",
    "\n",
    "        producer.poll(0)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging.info(\"Shutting down...\")\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message:\n",
      "{\n",
      "  \"transaction_id\": \"73173b37-edad-4e17-9fe1-48305b1a2410\",\n",
      "  \"timestamp\": \"2025-04-20T15:43:30.033099Z\",\n",
      "  \"customer_id\": \"cust_222\",\n",
      "  \"merchant_id\": \"merch_28\",\n",
      "  \"merchant_category\": \"entertainment\",\n",
      "  \"payment_method\": \"online\",\n",
      "  \"amount\": 1459060,\n",
      "  \"location\": {\n",
      "    \"lat\": 35.73375549866577,\n",
      "    \"lng\": 51.30714798144749\n",
      "  },\n",
      "  \"device_info\": {\n",
      "    \"os\": \"iOS\",\n",
      "    \"app_version\": \"3.1.0\",\n",
      "    \"device_model\": \"iPhone 15\"\n",
      "  },\n",
      "  \"status\": \"approved\",\n",
      "  \"commission_type\": \"flat\",\n",
      "  \"commission_amount\": 29181,\n",
      "  \"vat_amount\": 131315,\n",
      "  \"total_amount\": 1619556,\n",
      "  \"customer_type\": \"business\",\n",
      "  \"risk_level\": 1,\n",
      "  \"failure_reason\": null\n",
      "}\n",
      "Validation Errors: ['ERR_TIME']\n"
     ]
    }
   ],
   "source": [
    "consumer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'darooghe-consumer',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "}\n",
    "consumer = Consumer(consumer_conf)\n",
    "consumer.subscribe(['darooghe.transactions'])\n",
    "\n",
    "try:\n",
    "    msg = consumer.poll(timeout=10.0)\n",
    "    if msg is None:\n",
    "        print(\"No message received.\")\n",
    "    elif msg.error():\n",
    "        print(f\"Error: {msg.error()}\")\n",
    "    else:\n",
    "        event = json.loads(msg.value().decode('utf-8'))\n",
    "        # raw_event = json.loads(msg.value().decode('utf-8'))\n",
    "        # event = parse_transaction(raw_event)\n",
    "        print(\"Received message:\")\n",
    "        print(json.dumps(event, indent=2))\n",
    "\n",
    "        errors = validate_transaction(event)\n",
    "        if errors:\n",
    "            print(\"Validation Errors:\", errors)\n",
    "        else:\n",
    "            print(\"Message is VALID!\")\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function takes a raw JSON-like event (raw_event) from Kafka and:\n",
    "\n",
    "- **Extracts** each expected field (IDs, categories, status, etc.).\n",
    "\n",
    "- **Parses** the ISO-formatted timestamp (stripping the trailing “Z” and re-serializing to ISO).\n",
    "\n",
    "- **Casts** numeric values (amounts, VAT, risk level) to int and coordinates to float.\n",
    "\n",
    "- **Preserves** optional fields (device_info, failure_reason) with sensible defaults.\n",
    "\n",
    "- **Returns** a normalized dict with all fields in the right Python types.\n",
    "\n",
    "- **Handles** missing or malformed data by catching KeyError, ValueError, or TypeError, logging a schema error, and returning None so invalid events can be skipped or re-published to an error topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transaction(raw_event):\n",
    "    try:\n",
    "        event = {}\n",
    "        event['transaction_id'] = str(raw_event['transaction_id'])\n",
    "        event['timestamp'] = datetime.datetime.fromisoformat(\n",
    "            raw_event['timestamp'].replace('Z', '')\n",
    "        ).isoformat()\n",
    "        event['customer_id'] = str(raw_event['customer_id'])\n",
    "        event['merchant_id'] = str(raw_event['merchant_id'])\n",
    "        event['merchant_category'] = str(raw_event['merchant_category'])\n",
    "        event['payment_method'] = str(raw_event['payment_method'])\n",
    "        event['amount'] = int(raw_event['amount'])\n",
    "        event['location'] = {\n",
    "            'lat': float(raw_event['location']['lat']),\n",
    "            'lng': float(raw_event['location']['lng']),\n",
    "        }\n",
    "        event['device_info'] = raw_event.get('device_info', {})\n",
    "        event['status'] = str(raw_event['status'])\n",
    "        event['commission_type'] = str(raw_event['commission_type'])\n",
    "        event['commission_amount'] = int(raw_event['commission_amount'])\n",
    "        event['vat_amount'] = int(raw_event['vat_amount'])\n",
    "        event['total_amount'] = int(raw_event['total_amount'])\n",
    "        event['customer_type'] = str(raw_event['customer_type'])\n",
    "        event['risk_level'] = int(raw_event['risk_level'])\n",
    "        event['failure_reason'] = raw_event.get('failure_reason', None)\n",
    "        return event\n",
    "    except (KeyError, ValueError, TypeError) as e:\n",
    "        print(f\"Schema error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data from Kafka (once) and save to a local file (e.g., JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = Consumer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'batch-job-consumer',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "})\n",
    "consumer.subscribe(['darooghe.transactions'])\n",
    "\n",
    "with open('transactions_data.json', 'w') as f:\n",
    "    for _ in range(10000):\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None or msg.error():\n",
    "            continue\n",
    "        raw_event = msg.value().decode('utf-8')\n",
    "        f.write(raw_event + '\\n')\n",
    "\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PySpark to analyze that saved file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed metrics are defined by the following formulas:\n",
    "\n",
    "- **Total Commission** per category:  \n",
    "  $$ \\text{total\\_commission} = \\sum_{i=1}^{N} \\text{commission\\_amount}_i $$\n",
    "\n",
    "- **Average Commission** per transaction:  \n",
    "  $$ \\text{avg\\_commission} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{commission\\_amount}_i $$\n",
    "\n",
    "- **Transaction Count**:  \n",
    "  $$ \\text{transaction\\_count} = N $$\n",
    "\n",
    "- **Average Commission Ratio**:  \n",
    "  $$ \\text{avg\\_commission\\_ratio} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\text{commission\\_amount}_i}{\\text{total\\_amount}_i} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/26 14:55:26 WARN Utils: Your hostname, amir-Lenovo resolves to a loopback address: 127.0.1.1; using 192.168.217.64 instead (on interface wlp8s0)\n",
      "25/04/26 14:55:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/26 14:55:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+------------------+-----------------+--------------------+\n",
      "|merchant_category|total_commission|avg_commission    |transaction_count|avg_commission_ratio|\n",
      "+-----------------+----------------+------------------+-----------------+--------------------+\n",
      "|retail           |41004263        |20563.82296890672 |1994             |0.018017193139572094|\n",
      "|entertainment    |41893100        |20515.71988246817 |2042             |0.018017257715458642|\n",
      "|food_service     |41074509        |20881.804270462635|1967             |0.018017220475492927|\n",
      "|government       |41840033        |20784.914555389965|2013             |0.018017223352158204|\n",
      "|transportation   |41127958        |20729.81754032258 |1984             |0.018017249509518204|\n",
      "+-----------------+----------------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CommissionAnalysisBatchJob\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "transactions_df = spark.read.json(\"transactions_data.json\")\n",
    "\n",
    "aggregated_metrics = transactions_df.groupBy(\"merchant_category\").agg(\n",
    "    _sum(\"commission_amount\").alias(\"total_commission\"),\n",
    "    avg(\"commission_amount\").alias(\"avg_commission\"),\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    avg(col(\"commission_amount\") / col(\"total_amount\")).alias(\"avg_commission_ratio\")\n",
    ")\n",
    "\n",
    "aggregated_metrics.show(truncate=False)\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load historical data**  \n",
    "   We read all past transactions from the JSON file into a Spark DataFrame.\n",
    "\n",
    "2. **Define commission models**  \n",
    "\n",
    "   - **Flat**: a constant 2% rate.  \n",
    "\n",
    "   - **Progressive**: tiered two-rate model (1.5% up to 1 000 000; 3% thereafter). \n",
    "\n",
    "   - **Tiered**: base 2% plus a bonus 0.5% on any amount above 1 500 000.\n",
    "\n",
    "3. **UDF registration**  \n",
    "\n",
    "   By wrapping each Python function in udf(..., DoubleType()), Spark can apply them across the cluster.\n",
    "\n",
    "4. **Simulate commissions**  \n",
    "\n",
    "   We add three new columns (flat_commission, etc.) to the DataFrame, each holding that model’s computed value for the transaction’s amount.\n",
    "\n",
    "5. **Aggregate results**  \n",
    "\n",
    "   Grouping by merchant_category, we sum:  \n",
    "\n",
    "   - The **actual** commissions already collected (commission_amount).  \n",
    "\n",
    "   - The **simulated** totals under each of the three models.  \n",
    "\n",
    "6. **Compare**  \n",
    "\n",
    "   The resulting table lets you see, for each merchant category, how much commission Darooghe actually earned versus what each model would have yielded—enabling you to identify the **optimal commission structure**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+--------------------+---------------------+--------------------+\n",
      "|merchant_category|actual_total_commission|sim_flat_total      |sim_progressive_total|sim_tiered_total    |\n",
      "+-----------------+-----------------------+--------------------+---------------------+--------------------+\n",
      "|retail           |41004263               |4.1005229539999984E7|5.405845412999993E7  |4.568187095999997E7 |\n",
      "|entertainment    |41893100               |4.189408534000007E7 |5.526702296999998E7  |4.637942272000006E7 |\n",
      "|food_service     |41074509               |4.107548939999999E7 |5.4409329929999955E7 |4.5779860334999956E7|\n",
      "|government       |41840033               |4.1841009099999994E7|5.5230637050000004E7 |4.6578242445E7      |\n",
      "|transportation   |41127958               |4.112893235999997E7 |5.416426744499999E7  |4.549861609499994E7 |\n",
      "+-----------------+-----------------------+--------------------+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CommissionModelSimulation\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"transactions_data.json\")\n",
    "\n",
    "\n",
    "def flat_commission(amount):\n",
    "    return amount * 0.02\n",
    "\n",
    "def progressive_commission(amount):\n",
    "    return amount * 0.015 if amount <= 1_000_000 else amount * 0.03\n",
    "\n",
    "def tiered_commission(amount):\n",
    "    base = amount * 0.02\n",
    "    extra = amount * 0.005 if amount > 1_500_000 else 0\n",
    "    return base + extra\n",
    "\n",
    "# Register UDF(user defined function)\n",
    "flat_udf = udf(flat_commission, DoubleType())\n",
    "progressive_udf = udf(progressive_commission, DoubleType())\n",
    "tiered_udf = udf(tiered_commission, DoubleType())\n",
    "\n",
    "# Apply each model\n",
    "simulated = df.withColumn(\"flat_commission\", flat_udf(col(\"amount\"))) \\\n",
    "              .withColumn(\"progressive_commission\", progressive_udf(col(\"amount\"))) \\\n",
    "              .withColumn(\"tiered_commission\", tiered_udf(col(\"amount\")))\n",
    "\n",
    "# Aggregate per merchant_category\n",
    "results = simulated.groupBy(\"merchant_category\").agg(\n",
    "    _sum(\"commission_amount\").alias(\"actual_total_commission\"),\n",
    "    _sum(\"flat_commission\").alias(\"sim_flat_total\"),\n",
    "    _sum(\"progressive_commission\").alias(\"sim_progressive_total\"),\n",
    "    _sum(\"tiered_commission\").alias(\"sim_tiered_total\")\n",
    ")\n",
    "\n",
    "\n",
    "results.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction Pattern Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the relationship between payment_method and amount to see if there's a pattern.\n",
    "\n",
    "Analyze how each payment_method differs in terms of transaction volume and amount—by grouping on payment_method we compute:\n",
    "\n",
    "- **num_transactions**: how often each method is used  \n",
    "\n",
    "- **total_amount**: overall spend per method  \n",
    "\n",
    "- **average_amount**: typical transaction size  \n",
    "\n",
    "- **stddev_amount**: variability in spend\n",
    "\n",
    "This lets us see if, for example, mobile payments tend to be smaller but more frequent, or POS transactions are higher-value but less common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+------------+------------------+-----------------+\n",
      "|payment_method|num_transactions|total_amount|average_amount    |stddev_amount    |\n",
      "+--------------+----------------+------------+------------------+-----------------+\n",
      "|online        |2498            |2515074463  |1006835.2534027222|570523.1693378846|\n",
      "|pos           |2490            |2629182500  |1055896.5863453816|563076.6858889284|\n",
      "|mobile        |2555            |2655173846  |1039206.9847358122|561859.2174693045|\n",
      "|nfc           |2457            |2547806478  |1036958.2735042735|569215.1451068346|\n",
      "+--------------+----------------+------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"TransactionPatternAnalysis\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"transactions_data.json\")\n",
    "\n",
    "payment_stats = df.groupBy(\"payment_method\").agg(\n",
    "    count(\"*\").alias(\"num_transactions\"),\n",
    "    _sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"amount\").alias(\"average_amount\"),\n",
    "    stddev(\"amount\").alias(\"stddev_amount\")\n",
    ")\n",
    "\n",
    "payment_stats.show(truncate=False)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the busiest time by:\n",
    "\n",
    "1. **Hour of day** \n",
    "\n",
    "   - Extract hour from each timestamp, group by hour, and count transactions (num_transactions).\n",
    "\n",
    "   - This shows you which hours (e.g., 14:00–15:00) see the most activity.\n",
    "\n",
    "2. **Day of week**  \n",
    "\n",
    "   - Extract day_of_week (e.g., Monday, Tuesday) from each timestamp, group by it, and count transactions.  \n",
    "\n",
    "   - This reveals which weekdays or weekends are the busiest.\n",
    "\n",
    "Together, these metrics pinpoint daily and weekly peak transaction periods for capacity planning and targeted promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions per hour of day:\n",
      "+----+----------------+\n",
      "|hour|num_transactions|\n",
      "+----+----------------+\n",
      "|   0|             380|\n",
      "|   1|             400|\n",
      "|   2|             469|\n",
      "|   3|             463|\n",
      "|   4|             447|\n",
      "|   5|             401|\n",
      "|   6|             408|\n",
      "|   7|             429|\n",
      "|   8|             387|\n",
      "|   9|             452|\n",
      "|  10|             413|\n",
      "|  11|             385|\n",
      "|  12|             423|\n",
      "|  13|             419|\n",
      "|  14|             401|\n",
      "|  15|             433|\n",
      "|  16|             418|\n",
      "|  17|             394|\n",
      "|  18|             399|\n",
      "|  19|             411|\n",
      "|  20|             385|\n",
      "|  21|             439|\n",
      "|  22|             410|\n",
      "|  23|             434|\n",
      "+----+----------------+\n",
      "\n",
      "Transactions per day of week:\n",
      "+-----------+----------------+\n",
      "|day_of_week|num_transactions|\n",
      "+-----------+----------------+\n",
      "|  Wednesday|            1425|\n",
      "|    Tuesday|            1452|\n",
      "|     Friday|            1414|\n",
      "|   Thursday|            1438|\n",
      "|   Saturday|            1419|\n",
      "|     Monday|            1439|\n",
      "|     Sunday|            1413|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"PeakTransactionTimes\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"transactions_data.json\")\n",
    "\n",
    "df = df.withColumn(\"hour\", hour(\"timestamp\")) \\\n",
    "       .withColumn(\"day_of_week\", date_format(\"timestamp\", \"EEEE\")) \n",
    "\n",
    "\n",
    "hourly_stats = df.groupBy(\"hour\").agg(\n",
    "    count(\"*\").alias(\"num_transactions\")\n",
    ").orderBy(\"hour\")\n",
    "\n",
    "daily_stats = df.groupBy(\"day_of_week\").agg(\n",
    "    count(\"*\").alias(\"num_transactions\")\n",
    ")\n",
    "\n",
    "print(\"Transactions per hour of day:\")\n",
    "hourly_stats.show(24)\n",
    "\n",
    "print(\"Transactions per day of week:\")\n",
    "daily_stats.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group customers by their transaction behavior, then:\n",
    "\n",
    "1. **Compute per-customer stats**  \n",
    "\n",
    "   - **num_transaction**: how many times they paid  \n",
    "\n",
    "   - **total_spen**: sum of all their transaction amounts  \n",
    "\n",
    "   - **avg_spen**: average transaction size  \n",
    "\n",
    "2. **Assign segments**  \n",
    "\n",
    "   - **power_user** if they made more than 19 transactions  \n",
    "\n",
    "   - **big_spender** if they spent over 22 000 000 IRR in total  \n",
    "\n",
    "   - **regular** otherwise  \n",
    "\n",
    "3. **Review top customers**  \n",
    "\n",
    "   - Order by total_spent descending and show the top 10 to see our highest-value segments.  \n",
    "\n",
    "This segmentation lets us tailor marketing or loyalty programs to frequent users (“power users”) and high-value customers (“big spenders”), while treating the rest as our baseline “regular” group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+------------------+-----------+\n",
      "|customer_id|num_transactions|total_spent|avg_spent         |segment    |\n",
      "+-----------+----------------+-----------+------------------+-----------+\n",
      "|cust_416   |21              |25672339   |1222492.3333333333|power_user |\n",
      "|cust_748   |19              |22247687   |1170930.894736842 |big_spender|\n",
      "|cust_140   |16              |21869193   |1366824.5625      |regular    |\n",
      "|cust_566   |16              |21178241   |1323640.0625      |regular    |\n",
      "|cust_951   |17              |21060735   |1238866.7647058824|regular    |\n",
      "|cust_531   |16              |20774359   |1298397.4375      |regular    |\n",
      "|cust_918   |17              |20569158   |1209950.4705882352|regular    |\n",
      "|cust_709   |18              |20010541   |1111696.7222222222|regular    |\n",
      "|cust_899   |17              |19680791   |1157693.5882352942|regular    |\n",
      "|cust_600   |19              |19632924   |1033311.7894736842|regular    |\n",
      "+-----------+----------------+-----------+------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CustomerSegmentation\").getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"transactions_data.json\")\n",
    "\n",
    "customer_stats = df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"num_transactions\"),\n",
    "    _sum(\"amount\").alias(\"total_spent\"),\n",
    "    avg(\"amount\").alias(\"avg_spent\")\n",
    ")\n",
    "\n",
    "segmented = customer_stats.withColumn(\n",
    "    \"segment\",\n",
    "    when(col(\"num_transactions\") > 19, \"power_user\")\n",
    "    .when(col(\"total_spent\") > 22_000_000, \"big_spender\")\n",
    "    .otherwise(\"regular\")\n",
    ")\n",
    "\n",
    "segmented.orderBy(\"total_spent\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By grouping on **merchant_category**, this snippet lets you compare:\n",
    "\n",
    "- **num_transactions**: How many transactions each category handles  \n",
    "\n",
    "- **avg_amount**: The typical transaction size per category  \n",
    "\n",
    "- **total_amount**: The overall spend flowing through each category  \n",
    "\n",
    "- **avg_risk_level**: The average fraud-risk score assigned to transactions in each category  \n",
    "\n",
    "Ordering by num_transactions shows you which merchant categories drive the most volume, while the other metrics reveal differences in spend size and risk profile—helping you identify, for example, that “food_service” may be high-volume but low-value, whereas “entertainment” sees fewer transactions at higher amounts and risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+------------------+------------+------------------+\n",
      "|merchant_category|num_transactions|avg_amount        |total_amount|avg_risk_level    |\n",
      "+-----------------+----------------+------------------+------------+------------------+\n",
      "|entertainment    |2042            |1025810.1209598433|2094704267  |2.0514201762977473|\n",
      "|government       |2013            |1039269.9726775957|2092050455  |2.0665673124689516|\n",
      "|retail           |1994            |1028215.3846539619|2050261477  |2.083751253761284 |\n",
      "|transportation   |1984            |1036515.4324596775|2056446618  |2.0403225806451615|\n",
      "|food_service     |1967            |1044115.1347229283|2053774470  |2.072191154041688 |\n",
      "+-----------------+----------------+------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MerchantCategoryComparison\").getOrCreate()\n",
    "df = spark.read.json(\"transactions_data.json\")\n",
    "\n",
    "category_stats = df.groupBy(\"merchant_category\").agg(\n",
    "    count(\"*\").alias(\"num_transactions\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\"),\n",
    "    _sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"risk_level\").alias(\"avg_risk_level\")\n",
    ")\n",
    "\n",
    "category_stats.orderBy(\"num_transactions\", ascending=False).show(truncate=False)\n",
    "\n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucket each transaction into a part of the day (morning, afternoon, evening, night) based on its hour, then compute:\n",
    "\n",
    "- **num_transactions**: how many transactions occur in each bucket  \n",
    "\n",
    "- **avg_amount**: the average transaction size in each bucket  \n",
    "\n",
    "This reveals which parts of the day—say, “morning” versus “evening”—drive the most activity and where transaction values tend to be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+------------------+\n",
      "| day_part|num_transactions|        avg_amount|\n",
      "+---------+----------------+------------------+\n",
      "|afternoon|            2488|1026659.3954983923|\n",
      "|  evening|            2478|1037492.3644067796|\n",
      "|  morning|            2875|1038253.6135652174|\n",
      "|    night|            2159|1036138.7183881426|\n",
      "+---------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PeakTransactionTimes\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"transactions_data.json\")\n",
    "\n",
    "df = df.withColumn(\"hour\", hour(\"timestamp\")) \\\n",
    "       .withColumn(\"day_part\",\n",
    "           when(col(\"hour\").between(5, 11), \"morning\")\n",
    "          .when(col(\"hour\").between(12, 17), \"afternoon\")\n",
    "          .when(col(\"hour\").between(18, 23), \"evening\")\n",
    "          .otherwise(\"night\")\n",
    "       )\n",
    "\n",
    "day_part_stats = df.groupBy(\"day_part\").agg(\n",
    "    count(\"*\").alias(\"num_transactions\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ")\n",
    "\n",
    "day_part_stats.orderBy(\"day_part\").show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By extracting the **date** from each transaction’s timestamp and then grouping by that date, this code computes:\n",
    "\n",
    "- **total_amount**: total spend per day  \n",
    "\n",
    "- **num_transactions**: number of transactions per day \n",
    "\n",
    "- **avg_amount**: average transaction size per day  \n",
    "\n",
    "Ordering by date shows us the daily time series, so we can easily see whether overall spending (or transaction volume/size) is trending up or down over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+------------------+\n",
      "|date      |total_amount|num_transactions|avg_amount        |\n",
      "+----------+------------+----------------+------------------+\n",
      "|2025-04-19|735824056   |728             |1010747.3296703297|\n",
      "|2025-04-20|1457558096  |1418            |1027897.1057827927|\n",
      "|2025-04-21|1496528030  |1445            |1035659.5363321799|\n",
      "|2025-04-22|1525335221  |1449            |1052681.3119392684|\n",
      "|2025-04-23|1510470003  |1435            |1052592.336585366 |\n",
      "|2025-04-24|1508634479  |1444            |1044760.7195290859|\n",
      "|2025-04-25|1433101752  |1410            |1016384.2212765957|\n",
      "|2025-04-26|679785650   |671             |1013093.3681073026|\n",
      "+----------+------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DailySpendingTrend\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"transactions_data.json\")\n",
    "\n",
    "df = df.withColumn(\"date\", to_date(\"timestamp\"))\n",
    "\n",
    "daily_trend = df.groupBy(\"date\").agg(\n",
    "    _sum(\"amount\").alias(\"total_amount\"),\n",
    "    count(\"*\").alias(\"num_transactions\"),\n",
    "    avg(\"amount\").alias(\"avg_amount\")\n",
    ")\n",
    "\n",
    "daily_trend.orderBy(\"date\").show(truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all transactions into MongoDB, while:\n",
    "\n",
    "- **Connecting** to a local MongoDB server and selecting the `darooghe_db` database and `transactions` collection.  \n",
    "\n",
    "- **Reading** each event from the `transactions_data.json` file line-by-line.  \n",
    "\n",
    "- **Parsing** the `timestamp` field to a Python `datetime` and **adding** a new `date` field (only the day part) to each event.\n",
    "\n",
    "- **Inserting** the modified event into MongoDB.\n",
    "\n",
    "The new `date` field acts as a **partition key**, making it easier and faster to query data by day (for example, fetching only today's or yesterday's transactions).\n",
    "\n",
    "This setup improves scalability and allows efficient filtering by time when working with large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into MongoDB\n"
     ]
    }
   ],
   "source": [
    "# first run \"mongod --dbpath ~/mongodb-data\" on cmd\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"darooghe_db\"]\n",
    "collection = db[\"transactions\"]\n",
    "\n",
    "with open(\"transactions_data.json\") as f:\n",
    "    for line in f:\n",
    "        event = json.loads(line)\n",
    "\n",
    "        timestamp = datetime.fromisoformat(event[\"timestamp\"].replace(\"Z\", \"\"))\n",
    "        event[\"timestamp\"] = timestamp\n",
    "        event[\"date\"] = timestamp.date().isoformat()  # for partitioning\n",
    "\n",
    "        collection.insert_one(event)\n",
    "\n",
    "print(\"Data inserted into MongoDB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to MongoDB and **enforce a data retention policy** by:\n",
    "\n",
    "- Defining a **cutoff_time** (current time minus 24 hours).\n",
    "\n",
    "- Deleting all transactions where the `timestamp` is **older than 24 hours**.\n",
    "\n",
    "- Printing how many outdated documents were removed.\n",
    "\n",
    "This keeps the database clean and ensures only recent, detailed transaction data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 18633 documents older than 2025-04-25T12:08:21.058660\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"darooghe_db\"]\n",
    "collection = db[\"transactions\"]\n",
    "\n",
    "# Define retention period: 24 hours (1 day)\n",
    "cutoff_time = datetime.utcnow() - timedelta(days=1)\n",
    "\n",
    "# Delete all with a timestamp older than the cutoff\n",
    "delete_result = collection.delete_many({\"timestamp\": {\"$lt\": cutoff_time}})\n",
    "print(f\"Deleted {delete_result.deleted_count} documents older than {cutoff_time.isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to MongoDB and **create an aggregated daily commission dataset** by:\n",
    "\n",
    "- Grouping transactions by `merchant_id` and `date`.\n",
    "\n",
    "- Calculating total commission, number of transactions, and total amount per merchant per day.\n",
    "\n",
    "- Saving the result into a new collection called `merchant_daily_commissions`.\n",
    "\n",
    "This aggregation summarizes detailed data and makes long-term trend analysis much faster.\n",
    "\n",
    "---\n",
    "\n",
    "#### To check the results:\n",
    "\n",
    "1. First, open your MongoDB shell by running:\n",
    "\n",
    "\n",
    "    ```bash\n",
    "\n",
    "    mongosh mongodb://localhost:27017\n",
    "\n",
    "    ```\n",
    "\n",
    "2. Then switch to your database:\n",
    "\n",
    "    ```mongodb\n",
    "\n",
    "    use darooghe_db\n",
    "\n",
    "    ```\n",
    "\n",
    "3. Finally, view a few documents from the new collection:\n",
    "\n",
    "    ```mongodb\n",
    "\n",
    "    db.merchant_daily_commissions.find().limit(5).pretty()\n",
    "\n",
    "    ```\n",
    "\n",
    "This will show you a clean JSON view of the first 5 aggregated records!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated commission data saved to 'merchant_daily_commissions' collection.\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"darooghe_db\"]\n",
    "\n",
    "# Aggregation pipeline to compute daily summary per merchant\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": {\n",
    "                \"merchant_id\": \"$merchant_id\",\n",
    "                \"date\": \"$date\"\n",
    "            },\n",
    "            \"total_commission\": {\"$sum\": \"$commission_amount\"},\n",
    "            \"transaction_count\": {\"$sum\": 1},\n",
    "            \"total_amount\": {\"$sum\": \"$amount\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"merchant_id\": \"$_id.merchant_id\",\n",
    "            \"date\": \"$_id.date\",\n",
    "            \"total_commission\": 1,\n",
    "            \"transaction_count\": 1,\n",
    "            \"total_amount\": 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$out\": \"merchant_daily_commissions\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "db.transactions.aggregate(pipeline)\n",
    "print(\"Aggregated commission data saved to 'merchant_daily_commissions' collection.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
